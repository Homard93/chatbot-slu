{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb78dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import speech_recognition as sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83add5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greetings', 'patterns': ['Hi', 'Hey', 'Hello', 'How are you?', 'Good day'], 'responses': [\"Hello, I'm the University Chatbot\"]}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", \"Thank's a lot!\"], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'closing', 'patterns': ['Bye', 'Goodbye'], 'responses': ['Goodbye']}, {'tag': 'email', 'patterns': ['What is the email address of Finance Office?', 'Finance Office email address', 'Email address Finance Office', 'Finance email', 'Email Finance'], 'responses': ['The email address of the Finance Office is<br><a>finance.ar@slu.edu.ph</a> <br> For online payments please email your reciept on this email: <br> <a>claims.payment.tuition@slu.edu.ph</a> <br> <a>finance.ar@slu.edu.ph</a><br> <a>claimspayment@slu.edu.ph</a>']}, {'tag': 'landline', 'patterns': ['What are the SLU landline numbers?', 'SLU landline numbers', 'Contact SLU', 'SLU numbers', 'SLU hotline'], 'responses': ['+(63) (74) 442 3043; 442 2793; 442 2193; 443 2001; 444 8246 to 48 (loc. 279)']}, {'tag': 'funny', 'patterns': ['Tell me a joke', 'Tell me something funny', 'Do you know a joke'], 'responses': ['Why did the hipster burn his mouth? He drank the coffee before it was cool.', 'What did the buffalo say when his son left for college? Bison.']}, {'tag': 'tuition', 'patterns': ['How much is the tuition fee', 'Tuition', 'tuition fee', 'SLU tuition'], 'responses': ['What school specifically are you pertaining to? <br> Schools in SLU: <br> School of Nursing (SON) <br> School of Engineering and Architecture (SEA) <br> School of Natural Sciences (SNS) <br> School of Medicine (SOM) <br> School of Accountancy, Management, Computing and Information Studies (SAMCIS) <br> School of Teacher Education and Liberal Arts (STELA) <br> School of Law (SOL)']}, {'tag': 'SON', 'patterns': ['Tuition Fee for SON', 'what is the tuition fee for School of Nursing?', 'SON tuition', 'Tuition SON', 'Nursing Tuition', 'Tuition Nursing', 'Nursing Student Tuition Fee'], 'responses': ['Bachelor of Science in Nursing: ₱37,000 - ₱41,000']}, {'tag': 'SEA', 'patterns': ['How much is the tuition fee for bsarchi?', 'How much is the tuition fee for bsche?', 'How much is the tuition fee for bsce?', 'How much is the tuition fee for bsee?', 'How much is the tuition fee for bsge?', 'How much is the tuition fee for bsie?', 'How much is the tuition fee for bsme?', 'How much is the tuition fee for bsem?', 'Tuition fee for SEA', 'What are the tuition fee for School of Engineering and Architecture', 'SEA tuition', 'Tuition SEA', 'Enginering Tuition Fee', 'Tuition fee for engineering student'], 'responses': ['Bachelor of Science in Achitecture: ₱29,000 - ₱33,000 <br> Bachelor of Science in Chemical Engineering: ₱24,000 - ₱28,000 <br> Bachelor of Science in Civil Engineering: ₱32,000 - ₱36,000 <br> Bachelor of Science in Electrical Engineering: ₱29,000 - ₱33,000 <br> Bachelor of Science in Electronices Engineering: ₱27,000 - ₱31,000 <br> Bachelor of Science in Geodetic Engineering: ₱29,000 - ₱33,000 <br> Bachelor of Science in Industrial Engineering: ₱24,000 - ₱28,000 <br> Bachelor of Science in Mechanical Engineering: ₱26,000 - ₱30,000 <br> Bachelor of Science in Mechatronics Engineering: ₱26,000 - ₱30,000 <br> Bachelor of Science in Mining Engineering: ₱27,000 - ₱31,000']}, {'tag': 'SNS', 'patterns': ['How much is the tuition fee for bs bio?', 'How much is the tuition fee for bsmls?', 'How much is the tuition fee for bs pharm?', 'How much is the tuition fee for bs rad?', 'Tuition fee SNS', 'what is the tuition fee for School of Natural Sciences?', 'SNS tuition', 'Tuition SNS', 'NATSCI Tuition fee', 'Tuition fee for NATSCI', 'Medtech student tuition fee'], 'responses': ['Bachelor of Science in Biology: ₱28,000 - ₱32,000 <br> Bachelor of Science in Medical Laboratory Science: ₱28,000 - ₱32,000 <br> Bachelor of Science in Pharmacy ₱32,000 - ₱36,000 <br> Bachelor of Science in Radiologic Technology: ₱28,000 - ₱32,000']}, {'tag': 'SOM', 'patterns': ['Tuition fee for SOM', 'what is the tuition for School of Medicine?', 'SOM tuition', 'Tuition SOM', 'Tuition fee for Medicine', 'Medicine Tuition'], 'responses': ['₱25,000 - ₱30,000']}, {'tag': 'SAMCIS', 'patterns': ['How much is the tuition fee for bshm', 'How much is the tuition fee for bsba-em', 'How much is the tuition fee for bsba finman', 'How much is the tuition fee for bsba-hrdm', 'How much is the tuition fee for bs entrep', 'How much is the tuition fee for bs ma', 'How much is the tuition fee for bsba mktg', 'How much is the tuition fee for bs ac', 'How much is the tuition fee for accountancy', 'How much is the tuition fee for bsit', 'How much is the tuition fee for bscs', 'How much is the tuition fee for bsit', 'How much is the tuition fee for IT', 'Tuition fee for SAMCIS', 'what is the tuition for School of Accountancy, Management, Computing and Information Studies?', 'SAMCIS tuition', 'Tuition SAMCIS', 'IT/CS tuition fee', 'Accountancy tuition fee', 'Management Tuition fee'], 'responses': ['Bachelor of Science in Accountancy: ₱29,000 - ₱33,000 <br> Bachelor of Science in Management Accounting: ₱29,000 - ₱33,000 <br> Bachelor of Science in Business Administration major in: <br> Financial Management with specialization in Business Analytics: ₱29,000 - ₱33,000 <br> Marketing Management with specialization in Business Analytics: ₱30,000 - ₱34,000 <br> Bachelor of Science in Entrepreneurship with specialization in Business Analytics: ₱29,000 - ₱33,000 <br> Bachelor of Science in Tourism Management: ₱30,000 - ₱34,000 <br> Bachelor of Science in Hospitality Management: ₱30,000 - ₱34,000 <br> Bachelor of Science in Computer Science: ₱39,000 - ₱42,000 <br> Bachelor of Science in Information Technology: ₱36,000 - ₱39,000']}, {'tag': 'STELA', 'patterns': ['How much is the tuition fee for beed', 'How much is the tuition fee for beced', 'How much is the tuition fee for bpe', 'How much is the tuition fee for bsed engl', 'How much is the tuition fee for bsed math', 'How much is the tuition fee for bsed science', 'How much is the tuition fee for bsed soc stud', 'How much is the tuition fee for ba com', 'How much is the tuition fee for ba philo', 'How much is the tuition fee for ba polit sci', 'How much is the tuition fee for bs pysch', 'How much is the tuition fee for bsswk', 'Tuition fee for STELA', 'What is the tuition fee for School of  of Teacher Education and Liberal Arts?', 'STELA tuition', 'Tuition STELA', 'Education Tuition fee', 'Pschology Tuition fee'], 'responses': ['Bachelor of Elementary Education: ₱28,000 - ₱32,000 <br> Bachelor of Secondary Education with areas of specialization in:<br> English: ₱24,000 - ₱28,000 <br> Mathematics: ₱24,000 - ₱28,000 <br> Science: ₱29,000 - ₱33,000 <br> Social Studies: ₱24,000 - ₱28,000 <br> Bachelor of Physical Education: ₱26,000 - ₱30,000 <br> Certificate in Teaching: ₱28,000 - ₱32,000 <br> Bachelor of Arts in Communication: ₱26,000 - ₱30,000 <br> Bachelor of Arts in Philosophy: ₱28,000 - ₱32,000 <br> Bachelor of Arts in Political Science: ₱26,000 - ₱30,000 <br> Bachelor of Science in Psychology: ₱29,000 - ₱33,000 <br> Bachelor of Science in Social Work: ₱24,000 - ₱28,000']}, {'tag': 'SOL', 'patterns': ['Tuition fee for SOL', 'what is the tuition for School of Law?', 'SOL tuition', 'Tuition SOL', 'Tuition fee for Law', 'Law student tuition fee'], 'responses': ['₱25,000 - ₱30,000']}, {'tag': 'mechanicaleng', 'patterns': ['Tuition Mechanical Engineering', 'what is the tuition for Mechanical Engineering?', 'Mechanical Engineering tuition', 'ME tuition fee', 'MecEng tuition fee'], 'responses': ['Bachelor of Science in Mechanical Engineering: 26,000 - 30,000']}, {'tag': 'EntranceExam', 'patterns': ['Is there an entrance exam for new applicants in SLU?', 'Entrance exam for new applicants', 'Exam for incoming freshies', 'Entrance exam for freshmen', 'Entrance exam', 'SLUCEE', 'Freshmen exam'], 'responses': ['SLU College Entrance Examination For Incoming Freshmen for more information go to <a>https://www.slu.edu.ph/admissionfreshmen-application-guidelines/</a>']}, {'tag': 'RetakeEntranceExam', 'patterns': ['What if I failed the SLU Entrance Examination, could I retake it?'], 'responses': ['Unfortunately, No', 'No, you cannot retake the SLU Entrance Examination']}, {'tag': 'RetakeEntranceExamtwo', 'patterns': ['What if I failed the SLU Entrance Examination?'], 'responses': ['Unfortunately, you cannot retake the SLU Entrance Examination']}, {'tag': 'MinGrade', 'patterns': ['Is there a minimum required grade to take the entrance examination?', 'Minimum grade for entrance examination of SLU', 'Passing grade for the entrance exam'], 'responses': ['Please visit this link for more information <a>https://www.slu.edu.ph/procedure-for-the-online-application-for-admission-of-new-students-for-the-2nd-semester-of-ay-2020-2021-as-of-january-04-2021/</a>']}, {'tag': 'SLUScholarship', 'patterns': ['Are there available scholarships offered by SLU', 'Scholarship offered by SLU', 'Procedure in applying scholarship', 'Scholarship'], 'responses': ['Please visit this link for more information <a>https://www.slu.edu.ph/procedure-for-the-online-application-for-admission-of-new-students-for-the-2nd-semester-of-ay-2020-2021-as-of-january-04-2021/</a>']}, {'tag': 'EnrollmentSOM', 'patterns': ['What are the processes for School of Medicine Application?', 'What is enrollment Process for School of Medicine?', 'Enrollment process for SOM', 'SOM enrollment'], 'responses': ['Please visit this link for more information: <a>https://www.slu.edu.ph/school-of-medicine-application-for-admission-ay-2021-2022/</a>']}, {'tag': 'EmailEnrollment', 'patterns': ['What email address could I contact about enrollment process?', 'Email address for Enrollment Process'], 'responses': ['Please contact this email address: admissions@slu.edu.ph']}, {'tag': 'ApplicantsRequirements', 'patterns': ['What are the requirements for new applicants in SLU?', 'Requirements for Freshmen', 'Freshmen Requirements'], 'responses': ['Click the link for more information <a>https://www.slu.edu.ph/admission/freshmen-application-guidelines/</a> and fill up the form here: <a>https://i2.slu.edu.ph/newapplication/apply.jsp</a>']}, {'tag': 'OldStudentRequirements', 'patterns': ['What are the requirements for existing students in SLU?', 'What are the Requirements for Old Student who will enroll?'], 'responses': ['Go to your slu portal then click the (Enrollment) on the navigation bar. There are steps and instructions there that you need to follow for your enrollment.']}, {'tag': 'EmailAddFinance', 'patterns': ['What is the email address of Finance Office?', 'Finance Office email address'], 'responses': ['Finance Office Email Address: <br>claims.payment.tuition@slu.edu.ph<br>finance.ar@slu.edu.ph<br>claimspayment@slu.edu.ph']}, {'tag': 'SLULandline', 'patterns': ['What are the SLU landline numbers?', 'available SLU landline numbers'], 'responses': [' SLU Landline Number: +(63) (74) 442 3043; 442 2793; 442 2193; 443 2001; 444 8246 to 48 (loc. 279)']}, {'tag': 'DeanOffice', 'patterns': [\"How can I contact the Dean's Office\", \"What are the Email address of the Dean's Office\"], 'responses': ['SAMCIS: samcisdean@slu.edu.ph <br> SEA: sasdean@slu.edu.ph<br> SOL: sol.admission@slu.edu.ph<br> SOM: somdean@slu.edu.ph<br> SON: sondean@slu.edu.ph<br> SNS: snsdean@slu.edu.ph<br> STELA: steladean@slu.edu.ph']}, {'tag': 'Payment', 'patterns': ['Pay', 'How will we pay for the tuition?', 'What is the mode of payment?', 'Process of payments'], 'responses': ['Are you currently enrolled in SLU?']}, {'tag': 'YeQuestion', 'patterns': ['Yes', 'Yes, I am currently enrolled.'], 'responses': ['Login to your portal then go to statement of accounts then look for dragonpay or apply for an installment using bukas']}, {'tag': 'NoQuestion', 'patterns': ['No', 'No, I am not yet enrolled.'], 'responses': ['You must first apply for an enrollment in SLU and pass the entrance exam. You can do this by visiting this website, https://i2.slu.edu.ph/newapplication/. Fill in the enrollment form and wait for you scheduled visit at the Main Campus, Bonificatio Street, Baguio City.']}, {'tag': 'Checklist', 'patterns': ['Do you have a courses checklist?', 'Checklist', 'Curiculim checklist', 'What are the curiculum checklist?', 'Do you have a curiculum checklist?', 'What are your curiculum checklist'], 'responses': ['What specific curiculum checklist are you looking for?']}, {'tag': 'ChecklistBSPOLSCI', 'patterns': ['Subjects to take in Political Science', 'Political science subjects', 'Course description in bs political science', 'Available subects in political science', 'BSPOLSCI', 'Pol sci'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Polictical Science by visiting this drive, <a href='https://drive.google.com/drive/folders/1ltItNmh__b2R2yuQAk5WwXAE2MHPJ9xK?usp=sharing'> click here...</a>\"]}, {'tag': 'ChecklistBSARCH', 'patterns': ['Subjects to take in Architecture', 'Architecture subjects', 'Course description in bs architecture', 'Available subects in architecture', 'bs arch', 'archi'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Architecture by visiting this drive, <a href='https://drive.google.com/drive/folders/1PrTIqlwrPrW8jTRqys_7pAMCxikeiCIT?usp=sharing'> click here...</a>\"]}, {'tag': 'ChecklistBSPHARM', 'patterns': ['Subjects to take in Pharmacy', 'pharmacy subjects', 'Course description in bs pharmacy', 'Available subects in pharmacy', 'bs parhm', 'pharm'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Pharmacy by visiting this drive, <a href='https://drive.google.com/drive/folders/1V5JxhuqizsMKC8LYxvjtOCed2K-Z0RJb?usp=sharing'> click here...</a>\"]}, {'tag': 'ChecklistBSIT', 'patterns': ['Subjects to take in IT', 'IT subjects', 'Course description in bs information technology', 'Available subects in IT', 'BSIT', 'IT', 'Information Technology'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Information Technology by visiting this drive, <a href='https://drive.google.com/drive/folders/136szNhGYT7ukdx5iqTtShvb-B3g0IbEp?usp=sharing'> click here...</a>\"]}, {'tag': 'ChecklistBSME', 'patterns': ['Subjects to take in Mechanical Engineering', 'Mechanical Engineer subjects', 'Course description in Mechanical engineering', 'Available subects in mechanical engineering', 'bsme', 'Mech eng'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Mechanical Engineering by visiting this drive, <a href='https://drive.google.com/drive/folders/1Ero3GKvdfh5K20hK06vS3NLcXFpPWElJ?usp=sharing'> click here...</a>\"]}, {'tag': 'ChecklistBSBIO', 'patterns': ['Subjects to take in Biology', 'Biology subjects', 'Course description in bs biology', 'Available subects in biology', 'bs bio', 'Bio'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Biology by visiting this drive, <a href='https://drive.google.com/drive/folders/1sKtis1zU8p6t5zXFphBa6FkPIppc0ZMl?usp=sharing'> click here...</a>\"]}, {'tag': 'ChecklistBSIE', 'patterns': ['Subjects to take in Industrial Engineering', 'Industrial subjects', 'Course description in bs industrial engineering', 'Available subects in industrial engineering', 'bsie', 'Industrial eng'], 'responses': [\"You can view curiculum checklist for Banchelor of Science in Industrial Engineering by visiting this drive, <a href='https://drive.google.com/drive/folders/12s1u2AqS8uyh7ujf-uSUE7QhgOaMd4Yu?usp=sharing'> click here...</a>\"]}, {'tag': 'LookingFor,', 'patterns': [' '], 'responses': ['What are you looking for?']}, {'tag': 'Checklist', 'patterns': ['Do you have a courses checklist?', 'Checklist', 'Curiculim checklist', 'What are the curiculum checklist?', 'Do you have a curiculum checklist?', 'What are your curiculum checklist'], 'responses': ['What specifically curiculum checklist are you looking for?', 'What course are you looking for?']}, {'tag': 'default', 'patterns': ['What is', 'What are', 'What do', 'Do you', 'What does', 'The', 'no', 'yes', ' '], 'responses': [\"Sorry, I can't understand your query.\", 'What are you looking for?']}]}\n"
     ]
    }
   ],
   "source": [
    "# used a dictionary to represent an intents JSON file\n",
    "with open(\"../training_data/intents.json\", \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee9f388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'a', 'about', 'ac', 'accountancy', 'address', 'am', 'an', 'and', 'applicant', 'application', 'applying', 'arch', 'archi', 'architecture', 'are', 'art', 'available', 'b', 'ba', 'beced', 'beed', 'bio', 'biology', 'bpe', 'bsarchi', 'bsba', 'bsba-em', 'bsba-hrdm', 'bsce', 'bsche', 'bscs', 'bsed', 'bsee', 'bsem', 'bsge', 'bshm', 'bsie', 'bsit', 'bsme', 'bsmls', 'bspolsci', 'bsswk', 'by', 'bye', 'can', 'checklist', 'com', 'computing', 'contact', 'could', 'course', 'curiculim', 'curiculum', 'currently', 'day', 'dean', 'description', 'do', 'doe', 'education', 'email', 'eng', 'engineer', 'engineering', 'enginering', 'engl', 'enroll', 'enrolled', 'enrollment', 'entrance', 'entrep', 'exam', 'examination', 'existing', 'failed', 'fee', 'finance', 'finman', 'for', 'freshies', 'freshman', 'funny', 'good', 'goodbye', 'grade', 'have', 'hello', 'helpful', 'hey', 'hi', 'hotline', 'how', 'i', 'if', 'in', 'incoming', 'industrial', 'information', 'is', 'it', 'it/cs', 'joke', 'know', 'landline', 'law', 'liberal', 'lot', 'ma', 'management', 'math', 'me', 'meceng', 'mech', 'mechanical', 'medicine', 'medtech', 'minimum', 'mktg', 'mode', 'much', 'natsci', 'natural', 'new', 'no', 'not', 'number', 'nursing', 'of', 'offered', 'office', 'old', 'parhm', 'passing', 'pay', 'payment', 'pharm', 'pharmacy', 'philo', 'pol', 'polit', 'political', 'procedure', 'process', 'pschology', 'pysch', 'rad', 'required', 'requirement', 'retake', 'samcis', 'scholarship', 'school', 'sci', 'science', 'sea', 'slu', 'slucee', 'sn', 'soc', 'sol', 'som', 'something', 'son', 'stela', 'stud', 'student', 'study', 'subects', 'subject', 'take', 'teacher', 'technology', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'to', 'tuition', 'we', 'what', 'who', 'will', 'yes', 'yet', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# initializing lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Each list to create\n",
    "words = []\n",
    "classes = []\n",
    "doc_X = []\n",
    "doc_y = []\n",
    "# Loop through all the intents\n",
    "# tokenize each pattern and append tokens to words, the patterns and\n",
    "# the associated tag to their associated list\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        doc_X.append(pattern)\n",
    "        doc_y.append(intent[\"tag\"])\n",
    "    \n",
    "    # add the tag to the classes if it's not there already \n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "# lemmatize all the words in the vocab and convert them to lowercase\n",
    "# if the words don't appear in punctuation\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dee0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for training data\n",
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "# creating the bag of words model\n",
    "for idx, doc in enumerate(doc_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    # mark the index of class that the current pattern is associated\n",
    "    # to\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(doc_y[idx])] = 1\n",
    "    # add the one hot encoded BoW and associated classes to training \n",
    "    training.append([bow, output_row])\n",
    "# shuffle the data and convert it to an array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "# split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d310ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               24320     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                2600      \n",
      "=================================================================\n",
      "Total params: 35,176\n",
      "Trainable params: 35,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.5354 - accuracy: 0.0633\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.9975 - accuracy: 0.1176\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5696 - accuracy: 0.2172\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.2882 - accuracy: 0.3258\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9063 - accuracy: 0.4615\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.5012 - accuracy: 0.5611\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.3172 - accuracy: 0.6244\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0038 - accuracy: 0.7014\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.8721 - accuracy: 0.7647\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.8330 - accuracy: 0.7647\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7508 - accuracy: 0.7557\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7200 - accuracy: 0.7647\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.8281\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.8733\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8688\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8778\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.8733\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8914\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3570 - accuracy: 0.8688\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8914\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8914\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.9276\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.9050\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.9050\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2668 - accuracy: 0.9140\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2596 - accuracy: 0.9005\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9140\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.9050\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9231\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.9140\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9412\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9276\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9095\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9276\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9276\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 0.9367\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.9186\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9050\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.9095\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9276\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.9186\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1441 - accuracy: 0.9457\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9276\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9367\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9367\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9140\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.9367\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9412\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.9276\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9276\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9140\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9321\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9502\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9186\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9548\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1656 - accuracy: 0.9186\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.9457\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9412\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9457\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9276\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.9276\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9186\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9638\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9367\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2179 - accuracy: 0.9412\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9321\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9457\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9548\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1684 - accuracy: 0.9367\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1602 - accuracy: 0.9457\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9457\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1654 - accuracy: 0.9321\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9231\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9231\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9186\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1697 - accuracy: 0.9276\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9412\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9095\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9321\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.9321\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9502\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9276\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.9412\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9276\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9548\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.9321\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.9321\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9457\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9231\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9412\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9276\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2327 - accuracy: 0.9140\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9412\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9140\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9231\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9457\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9186\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9457\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2509 - accuracy: 0.9095\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9186\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.9095\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.9050\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1810 - accuracy: 0.9321\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.9140\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9231\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9367\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9367\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9231\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9276\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9276\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9367\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9367\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9412\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9005\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9502\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9231\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.9050\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9729\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9457\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9231\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3186 - accuracy: 0.9050\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9367\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9412\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9367\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.9186\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9276\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9457\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9367\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9412\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9412\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9548\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9502\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9412\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9502\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1490 - accuracy: 0.9276\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.9005\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9367\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9457\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9186\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9321\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9095\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9548\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9321\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9457\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9457\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9367\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9412\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9367\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9186\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1053 - accuracy: 0.9502\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9367\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9412\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9412\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9593\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9457\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.9321\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9502\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9683\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9457\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9276\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9457\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9231\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9502\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9593\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9276\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9367\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.9367\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9367\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.9367\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9683\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9321\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.9186\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9457\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9186\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.9502\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9095\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9638\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9457\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9412\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1509 - accuracy: 0.9321\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.9321\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9321\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1681 - accuracy: 0.9548\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.9457\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4080 - accuracy: 0.9095\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9502\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9367\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9321\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9367\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9186\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9457\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9412\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.9140\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9548\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9186\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1748 - accuracy: 0.9457\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.9457\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9457\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.9502\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27ee4d3cb08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining some parameters\n",
    "input_shape = (len(train_X[0]),)\n",
    "output_shape = len(train_y[0])\n",
    "epochs = 200\n",
    "# the deep learning model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_shape, activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(x=train_X, y=train_y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d178a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "def clean_text(text): \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(text, vocab): \n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)\n",
    "\n",
    "def pred_class(text): \n",
    "    bow = bag_of_words(text, words)\n",
    "    results = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.2\n",
    "    y_pred = [[idx, res] for idx, res in enumerate(results) if res > thresh]\n",
    "\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    return return_list\n",
    "\n",
    "def get_response(text, userID='123'): \n",
    "    results = pred_class(text)\n",
    "    while results:\n",
    "        for i in data[\"intents\"]: \n",
    "            if i[\"tag\"] == results[0][0]:\n",
    "                if \"context_set\" in i:\n",
    "                    context[userID] = i[\"context_set\"]\n",
    "\n",
    "                if not \"context_filter\" in i or (userID in context and \"context_filter\" in i and i[\"context_filter\"] == context[userID]):\n",
    "                    return random.choice(i[\"responses\"])\n",
    "\n",
    "        results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87915e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Jul/2021 13:53:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Jul/2021 13:53:15] \"GET /static/styles/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [21/Jul/2021 13:53:17] \"GET /get?msg=no HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Jul/2021 13:53:21] \"GET /get?msg=what%20is%20dog HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Jul/2021 13:53:30] \"GET /get?msg=what%20are%20courses%20you%20offer%3F HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for \n",
    "\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "    \n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('msg')\n",
    "    return str(get_response(userText))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d607b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f86246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52f2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
